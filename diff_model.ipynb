{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9290b9f4",
   "metadata": {},
   "source": [
    "## Generative AI Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e02c19",
   "metadata": {},
   "source": [
    "### Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9955b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2c3e23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|██████████| 2890/2890 [00:40<00:00, 71.15it/s]\n",
      "Processing val: 100%|██████████| 361/361 [00:05<00:00, 62.91it/s]\n",
      "Processing test: 100%|██████████| 362/362 [00:05<00:00, 67.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "input_dir = \"./VocalSet/FULL/\"  # <- change this\n",
    "output_dir = \"VocalSet_processed\"\n",
    "sample_rate = 22050\n",
    "duration = 4.0  # seconds\n",
    "split_ratio = [0.8, 0.1, 0.1]  # train, val, test\n",
    "\n",
    "# Create output folders\n",
    "splits = ['train', 'val', 'test']\n",
    "for split in splits:\n",
    "    os.makedirs(os.path.join(output_dir, split), exist_ok=True)\n",
    "\n",
    "# Gather all wav files\n",
    "all_files = []\n",
    "for root, _, files in os.walk(input_dir):\n",
    "    for f in files:\n",
    "        if f.endswith('.wav'):\n",
    "            full_path = os.path.join(root, f)\n",
    "            all_files.append(full_path)\n",
    "\n",
    "random.shuffle(all_files)\n",
    "n_total = len(all_files)\n",
    "n_train = int(split_ratio[0] * n_total)\n",
    "n_val = int(split_ratio[1] * n_total)\n",
    "\n",
    "# Split files\n",
    "train_files = all_files[:n_train]\n",
    "val_files = all_files[n_train:n_train + n_val]\n",
    "test_files = all_files[n_train + n_val:]\n",
    "\n",
    "split_map = {\n",
    "    \"train\": train_files,\n",
    "    \"val\": val_files,\n",
    "    \"test\": test_files\n",
    "}\n",
    "\n",
    "# Process files\n",
    "for split in splits:\n",
    "    for file_path in tqdm(split_map[split], desc=f\"Processing {split}\"):\n",
    "        try:\n",
    "            y, _ = librosa.load(file_path, sr=sample_rate)\n",
    "            y = librosa.util.fix_length(data=y, size=int(sample_rate * duration))\n",
    "            filename = os.path.basename(file_path)\n",
    "            sf.write(os.path.join(output_dir, split, filename), y, sample_rate)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35581d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: 100%|██████████| 181/181 [16:24<00:00,  5.44s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30: 100%|██████████| 181/181 [10:50<00:00,  3.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30: 100%|██████████| 181/181 [10:20<00:00,  3.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30: 100%|██████████| 181/181 [10:27<00:00,  3.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30: 100%|██████████| 181/181 [09:41<00:00,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30: 100%|██████████| 181/181 [09:52<00:00,  3.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30: 100%|██████████| 181/181 [10:09<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30: 100%|██████████| 181/181 [09:54<00:00,  3.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30: 100%|██████████| 181/181 [10:10<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30: 100%|██████████| 181/181 [10:55<00:00,  3.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30: 100%|██████████| 181/181 [11:10<00:00,  3.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30: 100%|██████████| 181/181 [11:28<00:00,  3.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30: 100%|██████████| 181/181 [11:18<00:00,  3.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30: 100%|██████████| 181/181 [10:27<00:00,  3.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30: 100%|██████████| 181/181 [10:26<00:00,  3.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30: 100%|██████████| 181/181 [13:26<00:00,  4.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30: 100%|██████████| 181/181 [15:22<00:00,  5.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30: 100%|██████████| 181/181 [13:29<00:00,  4.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30: 100%|██████████| 181/181 [12:16<00:00,  4.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30: 100%|██████████| 181/181 [11:12<00:00,  3.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30: 100%|██████████| 181/181 [10:59<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30: 100%|██████████| 181/181 [10:37<00:00,  3.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30: 100%|██████████| 181/181 [10:27<00:00,  3.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30: 100%|██████████| 181/181 [10:11<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30: 100%|██████████| 181/181 [10:34<00:00,  3.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30: 100%|██████████| 181/181 [10:58<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30: 100%|██████████| 181/181 [10:58<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30: 100%|██████████| 181/181 [10:58<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/30: 100%|██████████| 181/181 [10:18<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/30: 100%|██████████| 181/181 [09:54<00:00,  3.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0013\n",
      "✅ Models saved!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ----------------------------\n",
    "# Dataset Class\n",
    "# ----------------------------\n",
    "class VocalSetDataset(Dataset):\n",
    "    def __init__(self, directory, sample_rate=22050, duration=4.0):\n",
    "        self.files = glob(os.path.join(directory, \"*.wav\"))\n",
    "        self.sample_rate = sample_rate\n",
    "        self.target_len = int(sample_rate * duration)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio, sr = torchaudio.load(self.files[idx])\n",
    "        audio = torchaudio.functional.resample(audio, sr, self.sample_rate)\n",
    "        audio = audio[:, :self.target_len]\n",
    "        if audio.shape[1] < self.target_len:\n",
    "            pad = self.target_len - audio.shape[1]\n",
    "            audio = torch.nn.functional.pad(audio, (0, pad))\n",
    "        return audio.squeeze(0)\n",
    "\n",
    "# ----------------------------\n",
    "# Encoder and Decoder\n",
    "# ----------------------------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim=22050*4, latent_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=128, output_dim=22050*4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ----------------------------\n",
    "# Training Loop\n",
    "# ----------------------------\n",
    "def train_autoencoder(train_loader, val_loader, epochs=30, lr=1e-4):\n",
    "    encoder = Encoder().to(device)\n",
    "    decoder = Decoder().to(device)\n",
    "    optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            batch = batch.to(device)\n",
    "            batch = batch.view(batch.size(0), -1)\n",
    "            z = encoder(batch)\n",
    "            recon = decoder(z)\n",
    "            loss = loss_fn(recon, batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        print(f\"Train Loss: {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Save models\n",
    "    os.makedirs(\"saved_models\", exist_ok=True)\n",
    "    torch.save(encoder.state_dict(), \"saved_models/encoder.pth\")\n",
    "    torch.save(decoder.state_dict(), \"saved_models/decoder.pth\")\n",
    "    print(\"✅ Models saved!\")\n",
    "\n",
    "# ----------------------------\n",
    "# Run Training\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    train_dataset = VocalSetDataset(\"VocalSet_processed/train\")\n",
    "    val_dataset = VocalSetDataset(\"VocalSet_processed/val\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    train_autoencoder(train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9ce11f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting latents: 100%|██████████| 181/181 [01:17<00:00,  2.34it/s]\n",
      "Epoch 1/20: 100%|██████████| 91/91 [00:00<00:00, 159.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 91/91 [00:00<00:00, 175.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.9690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 91/91 [00:00<00:00, 182.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.8980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 91/91 [00:00<00:00, 185.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.7953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 91/91 [00:00<00:00, 173.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.7186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 91/91 [00:00<00:00, 184.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.6582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 91/91 [00:00<00:00, 185.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.6157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 91/91 [00:00<00:00, 193.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.5710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 91/91 [00:00<00:00, 189.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.5307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 91/91 [00:00<00:00, 180.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.5008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 91/91 [00:00<00:00, 184.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: 0.4715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 91/91 [00:00<00:00, 193.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Loss: 0.4297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 91/91 [00:00<00:00, 189.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss: 0.4111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 91/91 [00:00<00:00, 186.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Loss: 0.3866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 91/91 [00:00<00:00, 182.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Loss: 0.3641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 91/91 [00:00<00:00, 164.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Loss: 0.3414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 91/91 [00:00<00:00, 177.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Loss: 0.3155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 91/91 [00:00<00:00, 173.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Loss: 0.3076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 91/91 [00:00<00:00, 161.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Loss: 0.2858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 91/91 [00:00<00:00, 159.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Loss: 0.2735\n",
      "✅ Diffusion model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------\n",
    "# DDPM Noise Scheduler\n",
    "# --------------------------\n",
    "class DiffusionScheduler:\n",
    "    def __init__(self, timesteps=1000, beta_start=1e-4, beta_end=0.02):\n",
    "        self.timesteps = timesteps\n",
    "        self.beta = torch.linspace(beta_start, beta_end, timesteps)\n",
    "        self.alpha = 1.0 - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "\n",
    "    def q_sample(self, x_start, t, noise):\n",
    "        sqrt_alpha_hat = self.alpha_hat[t].sqrt().unsqueeze(1)\n",
    "        sqrt_one_minus_alpha_hat = (1 - self.alpha_hat[t]).sqrt().unsqueeze(1)\n",
    "        return sqrt_alpha_hat * x_start + sqrt_one_minus_alpha_hat * noise\n",
    "\n",
    "# --------------------------\n",
    "# Simple UNet-style MLP\n",
    "# --------------------------\n",
    "class MLPDiffusion(nn.Module):\n",
    "    def __init__(self, latent_dim=128, t_emb_dim=64):\n",
    "        super().__init__()\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(1, t_emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(t_emb_dim, t_emb_dim),\n",
    "        )\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim + t_emb_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t = t.float().unsqueeze(1) / 1000.0\n",
    "        t_emb = self.time_embed(t)\n",
    "        x = torch.cat([x, t_emb], dim=1)\n",
    "        return self.net(x)\n",
    "\n",
    "# --------------------------\n",
    "# Latent Dataset\n",
    "# --------------------------\n",
    "class LatentVocalSet(Dataset):\n",
    "    def __init__(self, dataloader, encoder, device):\n",
    "        self.latents = []\n",
    "        encoder.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Extracting latents\"):\n",
    "                batch = batch.to(device).view(batch.size(0), -1)\n",
    "                z = encoder(batch)\n",
    "                self.latents.append(z.cpu())\n",
    "        self.latents = torch.cat(self.latents)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.latents)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.latents[idx]\n",
    "\n",
    "# --------------------------\n",
    "# Training Loop\n",
    "# --------------------------\n",
    "def train_diffusion(epochs=20, latent_dim=128):\n",
    "    # Load encoder\n",
    "    diff_encoder = Encoder().to(device)\n",
    "    diff_encoder.load_state_dict(torch.load(\"saved_models/encoder.pth\"))\n",
    "    diff_encoder.eval()\n",
    "\n",
    "    # Dataset\n",
    "    audio_ds = VocalSetDataset(\"VocalSet_processed/train\")\n",
    "    audio_loader = DataLoader(audio_ds, batch_size=16, shuffle=False)\n",
    "\n",
    "    latent_ds = LatentVocalSet(audio_loader, diff_encoder, device)\n",
    "    latent_loader = DataLoader(latent_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Model & scheduler\n",
    "    model = MLPDiffusion(latent_dim).to(device)\n",
    "    noise_scheduler = DiffusionScheduler()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    mse = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x_0 in tqdm(latent_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            x_0 = x_0.to(device)\n",
    "            noise = torch.randn_like(x_0)\n",
    "            t = torch.randint(0, noise_scheduler.timesteps, (x_0.shape[0],), device=device)\n",
    "            x_t = noise_scheduler.q_sample(x_0, t, noise)\n",
    "            noise_pred = model(x_t, t)\n",
    "            loss = mse(noise_pred, noise)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(latent_loader):.4f}\")\n",
    "\n",
    "    # Save the model\n",
    "    os.makedirs(\"saved_models\", exist_ok=True)\n",
    "    torch.save(model.state_dict(), \"saved_models/diffusion_model.pth\")\n",
    "    print(\"✅ Diffusion model saved!\")\n",
    "\n",
    "# --------------------------\n",
    "# Run Training\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    train_diffusion()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f96e03e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating sample 1: 1000it [00:00, 1508.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: generated_audio\\sample_1.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating sample 2: 1000it [00:00, 1595.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: generated_audio\\sample_2.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating sample 3: 1000it [00:00, 1711.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: generated_audio\\sample_3.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating sample 4: 1000it [00:00, 1602.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: generated_audio\\sample_4.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating sample 5: 1000it [00:00, 1525.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: generated_audio\\sample_5.wav\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --------------------------\n",
    "# Inference Function\n",
    "# --------------------------\n",
    "def generate_audio(\n",
    "    output_dir=\"generated_audio\",\n",
    "    num_samples=5,\n",
    "    latent_dim=128,\n",
    "    timesteps=1000,\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Load models\n",
    "    decoder = Decoder().to(device)\n",
    "    decoder.load_state_dict(torch.load(\"saved_models/decoder.pth\"))\n",
    "    decoder.eval()\n",
    "\n",
    "    diffusion = MLPDiffusion(latent_dim).to(device)\n",
    "    diffusion.load_state_dict(torch.load(\"saved_models/diffusion_model.pth\"))\n",
    "    diffusion.eval()\n",
    "\n",
    "    scheduler = DiffusionScheduler(timesteps)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            x_t = torch.randn(1, latent_dim).to(device)  # Start with noise\n",
    "\n",
    "            for t in tqdm(reversed(range(timesteps)), desc=f\"Generating sample {i+1}\"):\n",
    "                t_tensor = torch.tensor([t], device=device)\n",
    "                z_pred = diffusion(x_t, t_tensor)\n",
    "                beta_t = scheduler.beta[t].to(device)\n",
    "                alpha_t = scheduler.alpha[t].to(device)\n",
    "                alpha_hat_t = scheduler.alpha_hat[t].to(device)\n",
    "\n",
    "                if t > 0:\n",
    "                    noise = torch.randn_like(x_t)\n",
    "                else:\n",
    "                    noise = 0\n",
    "\n",
    "                x_t = (\n",
    "                    (1 / alpha_t.sqrt()) *\n",
    "                    (x_t - ((1 - alpha_t) / (1 - alpha_hat_t).sqrt()) * z_pred)\n",
    "                ) + beta_t.sqrt() * noise\n",
    "\n",
    "            # Decode latent to waveform\n",
    "            waveform = decoder(x_t).cpu().squeeze()\n",
    "            if waveform.ndim == 1:\n",
    "                waveform = waveform.unsqueeze(0)  # (1, samples)\n",
    "\n",
    "            # Save to .wav\n",
    "            out_path = os.path.join(output_dir, f\"sample_{i+1}.wav\")\n",
    "            torchaudio.save(out_path, waveform, sample_rate=16000)\n",
    "            print(f\"✅ Saved: {out_path}\")\n",
    "\n",
    "# --------------------------\n",
    "# Run\n",
    "# --------------------------\n",
    "generate_audio(num_samples=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c1dc702",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample 1: 1000it [00:00, 1422.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: generated_audio_improved\\sample_1.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample 2: 1000it [00:00, 1601.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: generated_audio_improved\\sample_2.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample 3: 1000it [00:00, 1543.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: generated_audio_improved\\sample_3.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample 4: 1000it [00:00, 1408.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: generated_audio_improved\\sample_4.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample 5: 1000it [00:00, 1625.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: generated_audio_improved\\sample_5.wav\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "class DiffusionScheduler:\n",
    "    def __init__(self, timesteps=1000, beta_start=1e-4, beta_end=0.02):\n",
    "        self.timesteps = timesteps\n",
    "        self.beta = torch.linspace(beta_start, beta_end, timesteps)\n",
    "        self.alpha = 1.0 - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "\n",
    "    def q_sample(self, x_start, t, noise):\n",
    "        sqrt_alpha_hat = self.alpha_hat[t].sqrt().unsqueeze(1)\n",
    "        sqrt_one_minus_alpha_hat = (1 - self.alpha_hat[t]).sqrt().unsqueeze(1)\n",
    "        return sqrt_alpha_hat * x_start + sqrt_one_minus_alpha_hat * noise\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=128, output_dim=22050*4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class MLPDiffusion(nn.Module):\n",
    "    def __init__(self, latent_dim=128, time_embed_dim=64):\n",
    "        super().__init__()\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(1, time_embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(time_embed_dim, time_embed_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim + time_embed_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t = t.float().unsqueeze(1) / 1000  # Normalize timestep\n",
    "        t_emb = self.time_embed(t)\n",
    "        x_t = torch.cat([x, t_emb], dim=1)\n",
    "        return self.net(x_t)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Re-initialize model objects\n",
    "decoder = Decoder().to(device)\n",
    "diffusion = MLPDiffusion().to(device)\n",
    "\n",
    "# Load weights\n",
    "decoder.load_state_dict(torch.load(\"saved_models/decoder.pth\", map_location=device))\n",
    "diffusion.load_state_dict(torch.load(\"saved_models/diffusion_model.pth\", map_location=device))\n",
    "scheduler = DiffusionScheduler(timesteps=1000)\n",
    "\n",
    "# ----- Utility: Post-Processing -----\n",
    "def high_pass_filter(waveform, cutoff=30, sr=16000):\n",
    "    b, a = butter(1, cutoff / (sr / 2), btype='high', analog=False)\n",
    "    return torch.tensor(lfilter(b, a, waveform.numpy()))\n",
    "\n",
    "def normalize(wav):\n",
    "    return wav / torch.max(torch.abs(wav))\n",
    "\n",
    "def post_process_waveform(waveform, sr=16000):\n",
    "    waveform = normalize(waveform)\n",
    "    waveform = high_pass_filter(waveform, sr=sr)\n",
    "    waveform = torch.clamp(waveform, -1.0, 1.0)\n",
    "    return waveform\n",
    "\n",
    "# ----- Improved Inference Function -----\n",
    "def improved_generate_audio(\n",
    "    decoder, diffusion, scheduler,\n",
    "    num_samples=5, latent_dim=128, timesteps=1000,\n",
    "    output_dir=\"generated_audio_improved\"\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    decoder.eval()\n",
    "    diffusion.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            x = torch.randn(1, latent_dim).to(device)\n",
    "\n",
    "            for t in tqdm(reversed(range(timesteps)), desc=f\"Sample {i+1}\"):\n",
    "                t_tensor = torch.tensor([t], device=device)\n",
    "                noise_pred = diffusion(x, t_tensor)\n",
    "\n",
    "                beta_t = scheduler.beta[t].to(device)\n",
    "                alpha_t = scheduler.alpha[t].to(device)\n",
    "                alpha_hat_t = scheduler.alpha_hat[t].to(device)\n",
    "\n",
    "                if t > 0:\n",
    "                    z = torch.randn_like(x)\n",
    "                else:\n",
    "                    z = 0\n",
    "\n",
    "                x = (\n",
    "                    (1 / alpha_t.sqrt()) *\n",
    "                    (x - ((1 - alpha_t) / (1 - alpha_hat_t).sqrt()) * noise_pred)\n",
    "                ) + beta_t.sqrt() * z\n",
    "\n",
    "            # Decode latent to waveform\n",
    "            waveform = decoder(x).cpu().squeeze()\n",
    "            if waveform.ndim == 1:\n",
    "                waveform = waveform.unsqueeze(0)\n",
    "\n",
    "            # Post-process\n",
    "            waveform = post_process_waveform(waveform)\n",
    "\n",
    "            # Save\n",
    "            out_path = os.path.join(output_dir, f\"sample_{i+1}.wav\")\n",
    "            torchaudio.save(out_path, waveform, sample_rate=16000)\n",
    "            print(f\"✅ Saved: {out_path}\")\n",
    "\n",
    "# Call this once everything is trained and loaded\n",
    "improved_generate_audio(\n",
    "    decoder=decoder,\n",
    "    diffusion=diffusion,\n",
    "    scheduler=scheduler,\n",
    "    num_samples=5\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
