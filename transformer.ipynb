{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e00aab43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader, random_split\n",
    "from torch.optim import Adam\n",
    "import torchaudio\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b0b6fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total audio files found: 3613\n",
      "Epoch 1/10 | Loss: 618283.0923\n",
      "Epoch 2/10 | Loss: 473917.1042\n",
      "Epoch 3/10 | Loss: 472727.5692\n",
      "Epoch 4/10 | Loss: 471568.9315\n",
      "Epoch 5/10 | Loss: 470609.7468\n",
      "Epoch 6/10 | Loss: 469200.2812\n",
      "Epoch 7/10 | Loss: 468357.2720\n",
      "Epoch 8/10 | Loss: 466873.3768\n",
      "Epoch 9/10 | Loss: 465593.8009\n",
      "Epoch 10/10 | Loss: 464874.2840\n",
      "Model saved as vqvae_model.pth\n",
      "Saved: samples/original_0.wav and samples/reconstructed_0.wav\n",
      "Saved: samples/original_1.wav and samples/reconstructed_1.wav\n",
      "Saved: samples/original_2.wav and samples/reconstructed_2.wav\n"
     ]
    }
   ],
   "source": [
    "# ========== Dataset ========== #\n",
    "class VocalSetDataset(Dataset):\n",
    "    def __init__(self, file_list):\n",
    "        self.files = file_list\n",
    "        self.mel_transform = nn.Sequential(\n",
    "            MelSpectrogram(sample_rate=16000, n_fft=1024, hop_length=256, n_mels=80),\n",
    "            AmplitudeToDB()\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        waveform, sr = torchaudio.load(self.files[idx])\n",
    "        if sr != 16000:\n",
    "            waveform = torchaudio.functional.resample(waveform, sr, 16000)\n",
    "        mel = self.mel_transform(waveform)\n",
    "        return mel.squeeze(0), waveform.squeeze(0)\n",
    "\n",
    "# ========== Load Dataset ========== #\n",
    "root_dir = 'VocalSet_processed'  # Update if needed\n",
    "\n",
    "# Recursively get all .wav files from subdirectories\n",
    "all_files = [\n",
    "    os.path.join(root, f)\n",
    "    for root, _, files in os.walk(root_dir)\n",
    "    for f in files if f.endswith('.wav')\n",
    "]\n",
    "\n",
    "print(\"Total audio files found:\", len(all_files))\n",
    "if len(all_files) == 0:\n",
    "    raise RuntimeError(\"No .wav files found. Please check the path and file extensions.\")\n",
    "\n",
    "dataset = VocalSetDataset(all_files)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True)\n",
    "\n",
    "# ========== VQ-VAE Model ========== #\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, num_embeddings=512, embedding_dim=64):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(80, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, embedding_dim, kernel_size=4, stride=2, padding=1)\n",
    "        )\n",
    "        self.codebook = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(embedding_dim, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(128, 80, kernel_size=4, stride=2, padding=1)\n",
    "        )\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_e = self.encoder(x)\n",
    "        z_e_perm = z_e.permute(0, 2, 1)  # B x T x C\n",
    "        distances = (\n",
    "            torch.sum(z_e_perm ** 2, dim=2, keepdim=True)\n",
    "            - 2 * torch.matmul(z_e_perm, self.codebook.weight.t())\n",
    "            + torch.sum(self.codebook.weight ** 2, dim=1)\n",
    "        )\n",
    "        encoding_indices = torch.argmin(distances, dim=-1)\n",
    "        z_q = self.codebook(encoding_indices)\n",
    "        z_q = z_q.permute(0, 2, 1)  # B x C x T\n",
    "\n",
    "        commitment_loss = F.mse_loss(z_e.detach(), z_q)\n",
    "        codebook_loss = F.mse_loss(z_e, z_q.detach())\n",
    "        loss = commitment_loss + codebook_loss\n",
    "\n",
    "        x_recon = self.decoder(z_q)\n",
    "        return x_recon, loss\n",
    "\n",
    "# ========== Train ========== #\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VQVAE().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "recon_loss_fn = nn.MSELoss()\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for mel, _ in train_loader:\n",
    "        mel = mel.to(device)\n",
    "        mel = mel[:, :, :256]  # Ensure fixed size input\n",
    "        x_recon, vq_loss = model(mel)\n",
    "        \n",
    "        # ðŸ”§ Crop output to match input size\n",
    "        min_len = min(x_recon.shape[2], mel.shape[2])\n",
    "        x_recon = x_recon[:, :, :min_len]\n",
    "        mel = mel[:, :, :min_len]\n",
    "\n",
    "        recon_loss = recon_loss_fn(x_recon, mel)\n",
    "        loss = recon_loss + vq_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {total_loss:.4f}\")\n",
    "\n",
    "\n",
    "# ========== Save Model ========== #\n",
    "torch.save(model.state_dict(), 'vqvae_model.pth')\n",
    "print(\"Model saved as vqvae_model.pth\")\n",
    "\n",
    "# ========== Generate Sample Reconstructions ========== #\n",
    "model.eval()\n",
    "os.makedirs(\"samples\", exist_ok=True)\n",
    "with torch.no_grad():\n",
    "    for i in range(3):  # Save 3 examples\n",
    "        mel, waveform = dataset[i]\n",
    "        mel = mel.unsqueeze(0).to(device)\n",
    "        mel = mel[:, :, :256]\n",
    "        recon_mel, _ = model(mel)\n",
    "        recon_mel = recon_mel.cpu().squeeze(0)\n",
    "\n",
    "        inverse_mel = torchaudio.transforms.InverseMelScale(n_stft=513, n_mels=80, sample_rate=16000)(recon_mel)\n",
    "        griffin_lim = torchaudio.transforms.GriffinLim(n_fft=1024, hop_length=256)\n",
    "        waveform_recon = griffin_lim(inverse_mel)\n",
    "\n",
    "        sf.write(f\"samples/original_{i}.wav\", waveform[:waveform_recon.shape[-1]].numpy(), 16000)\n",
    "        sf.write(f\"samples/reconstructed_{i}.wav\", waveform_recon.numpy(), 16000)\n",
    "        print(f\"Saved: samples/original_{i}.wav and samples/reconstructed_{i}.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5db1c2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total audio files found: 3613\n",
      "Epoch 1/10 | Loss: 614954.0660 | LR: 0.000200\n",
      "Epoch 2/10 | Loss: 470176.4475 | LR: 0.000200\n",
      "Epoch 3/10 | Loss: 468976.4622 | LR: 0.000200\n",
      "Epoch 4/10 | Loss: 467512.8056 | LR: 0.000200\n",
      "Epoch 5/10 | Loss: 466053.8285 | LR: 0.000100\n",
      "Epoch 6/10 | Loss: 464731.0772 | LR: 0.000100\n",
      "Epoch 7/10 | Loss: 464088.2267 | LR: 0.000100\n",
      "Epoch 8/10 | Loss: 463668.0990 | LR: 0.000100\n",
      "Epoch 9/10 | Loss: 463077.5611 | LR: 0.000100\n",
      "Epoch 10/10 | Loss: 462624.0186 | LR: 0.000050\n",
      "Model saved as vqvae_model.pth\n",
      "Saved: samples/original_0.wav and samples/reconstructed_0.wav\n",
      "Saved: samples/original_1.wav and samples/reconstructed_1.wav\n",
      "Saved: samples/original_2.wav and samples/reconstructed_2.wav\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import soundfile as sf\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "\n",
    "# ========== Dataset ========== #\n",
    "class VocalSetDataset(Dataset):\n",
    "    def __init__(self, file_list):\n",
    "        self.files = file_list\n",
    "        self.mel_transform = nn.Sequential(\n",
    "            MelSpectrogram(sample_rate=16000, n_fft=1024, hop_length=256, n_mels=80),\n",
    "            AmplitudeToDB()\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        waveform, sr = torchaudio.load(self.files[idx])\n",
    "        if sr != 16000:\n",
    "            waveform = torchaudio.functional.resample(waveform, sr, 16000)\n",
    "        mel = self.mel_transform(waveform)\n",
    "        return mel.squeeze(0), waveform.squeeze(0)\n",
    "\n",
    "# ========== Load Dataset ========== #\n",
    "root_dir = 'VocalSet_processed'  # Update if needed\n",
    "\n",
    "# Recursively get all .wav files from subdirectories\n",
    "all_files = [\n",
    "    os.path.join(root, f)\n",
    "    for root, _, files in os.walk(root_dir)\n",
    "    for f in files if f.endswith('.wav')\n",
    "]\n",
    "\n",
    "print(\"Total audio files found:\", len(all_files))\n",
    "if len(all_files) == 0:\n",
    "    raise RuntimeError(\"No .wav files found. Please check the path and file extensions.\")\n",
    "\n",
    "dataset = VocalSetDataset(all_files)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True)\n",
    "\n",
    "# ========== VQ-VAE Model ========== #\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, num_embeddings=512, embedding_dim=64):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(80, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, embedding_dim, kernel_size=4, stride=2, padding=1)\n",
    "        )\n",
    "        self.codebook = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(embedding_dim, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(128, 80, kernel_size=4, stride=2, padding=1)\n",
    "        )\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_e = self.encoder(x)\n",
    "        z_e_perm = z_e.permute(0, 2, 1)  # B x T x C\n",
    "        distances = (\n",
    "            torch.sum(z_e_perm ** 2, dim=2, keepdim=True)\n",
    "            - 2 * torch.matmul(z_e_perm, self.codebook.weight.t())\n",
    "            + torch.sum(self.codebook.weight ** 2, dim=1)\n",
    "        )\n",
    "        encoding_indices = torch.argmin(distances, dim=-1)\n",
    "        z_q = self.codebook(encoding_indices)\n",
    "        z_q = z_q.permute(0, 2, 1)  # B x C x T\n",
    "\n",
    "        commitment_loss = F.mse_loss(z_e.detach(), z_q)\n",
    "        codebook_loss = F.mse_loss(z_e, z_q.detach())\n",
    "        loss = commitment_loss + codebook_loss\n",
    "\n",
    "        x_recon = self.decoder(z_q)\n",
    "        return x_recon, loss\n",
    "\n",
    "# ========== Train ========== #\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VQVAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
    "recon_loss_fn = nn.MSELoss()\n",
    "\n",
    "# ========== Learning Rate Scheduler ========== #\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # Decay the LR every 5 epochs\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for mel, _ in train_loader:\n",
    "        mel = mel.to(device)\n",
    "        mel = mel[:, :, :256]  # Ensure fixed size input\n",
    "        x_recon, vq_loss = model(mel)\n",
    "        \n",
    "        # ðŸ”§ Crop output to match input size\n",
    "        min_len = min(x_recon.shape[2], mel.shape[2])\n",
    "        x_recon = x_recon[:, :, :min_len]\n",
    "        mel = mel[:, :, :min_len]\n",
    "\n",
    "        recon_loss = recon_loss_fn(x_recon, mel)\n",
    "        loss = recon_loss + vq_loss\n",
    "        \n",
    "        # Gradient Clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Step the learning rate scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {total_loss:.4f} | LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "# ========== Save Model ========== #\n",
    "torch.save(model.state_dict(), 'vqvae_model.pth')\n",
    "print(\"Model saved as vqvae_model.pth\")\n",
    "\n",
    "# ========== Generate Sample Reconstructions ========== #\n",
    "model.eval()\n",
    "os.makedirs(\"samples\", exist_ok=True)\n",
    "with torch.no_grad():\n",
    "    for i in range(3):  # Save 3 examples\n",
    "        mel, waveform = dataset[i]\n",
    "        mel = mel.unsqueeze(0).to(device)\n",
    "        mel = mel[:, :, :256]\n",
    "        recon_mel, _ = model(mel)\n",
    "        recon_mel = recon_mel.cpu().squeeze(0)\n",
    "\n",
    "        inverse_mel = torchaudio.transforms.InverseMelScale(n_stft=513, n_mels=80, sample_rate=16000)(recon_mel)\n",
    "        griffin_lim = torchaudio.transforms.GriffinLim(n_fft=1024, hop_length=256)\n",
    "        waveform_recon = griffin_lim(inverse_mel)\n",
    "\n",
    "        sf.write(f\"samples/original_{i}.wav\", waveform[:waveform_recon.shape[-1]].numpy(), 16000)\n",
    "        sf.write(f\"samples/reconstructed_{i}.wav\", waveform_recon.numpy(), 16000)\n",
    "        print(f\"Saved: samples/original_{i}.wav and samples/reconstructed_{i}.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7ed1412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total audio files found: 3613\n",
      "Epoch 1/10 | Loss: 665700.0164 | LR: 0.000200\n",
      "Epoch 2/10 | Loss: 475022.6128 | LR: 0.000200\n",
      "Epoch 3/10 | Loss: 473760.9290 | LR: 0.000200\n",
      "Epoch 4/10 | Loss: 472314.9242 | LR: 0.000200\n",
      "Epoch 5/10 | Loss: 470998.4531 | LR: 0.000100\n",
      "Epoch 6/10 | Loss: 469297.7219 | LR: 0.000100\n",
      "Epoch 7/10 | Loss: 468669.4342 | LR: 0.000100\n",
      "Epoch 8/10 | Loss: 468130.3535 | LR: 0.000100\n",
      "Epoch 9/10 | Loss: 467482.3655 | LR: 0.000100\n",
      "Epoch 10/10 | Loss: 467001.5427 | LR: 0.000050\n",
      "Model saved as vqvae_model.pth\n",
      "Saved: samples/original_0.wav and samples/reconstructed_0.wav\n",
      "Saved: samples/original_1.wav and samples/reconstructed_1.wav\n",
      "Saved: samples/original_2.wav and samples/reconstructed_2.wav\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import soundfile as sf\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "\n",
    "# ========== Dataset ========== #\n",
    "class VocalSetDataset(Dataset):\n",
    "    def __init__(self, file_list):\n",
    "        self.files = file_list\n",
    "        self.mel_transform = nn.Sequential(\n",
    "            MelSpectrogram(sample_rate=16000, n_fft=1024, hop_length=256, n_mels=80),\n",
    "            AmplitudeToDB()\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        waveform, sr = torchaudio.load(self.files[idx])\n",
    "        if sr != 16000:\n",
    "            waveform = torchaudio.functional.resample(waveform, sr, 16000)\n",
    "        mel = self.mel_transform(waveform)\n",
    "        return mel.squeeze(0), waveform.squeeze(0)\n",
    "\n",
    "# ========== Load Dataset ========== #\n",
    "root_dir = 'VocalSet_processed'  # Update if needed\n",
    "\n",
    "# Recursively get all .wav files from subdirectories\n",
    "all_files = [\n",
    "    os.path.join(root, f)\n",
    "    for root, _, files in os.walk(root_dir)\n",
    "    for f in files if f.endswith('.wav')\n",
    "]\n",
    "\n",
    "print(\"Total audio files found:\", len(all_files))\n",
    "if len(all_files) == 0:\n",
    "    raise RuntimeError(\"No .wav files found. Please check the path and file extensions.\")\n",
    "\n",
    "dataset = VocalSetDataset(all_files)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True)\n",
    "\n",
    "# ========== VQ-VAE Model ========== #\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, num_embeddings=512, embedding_dim=64):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(80, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, embedding_dim, kernel_size=4, stride=2, padding=1)\n",
    "        )\n",
    "        self.codebook = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(embedding_dim, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(128, 80, kernel_size=4, stride=2, padding=1)\n",
    "        )\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_e = self.encoder(x)\n",
    "        z_e_perm = z_e.permute(0, 2, 1)  # B x T x C\n",
    "        distances = (\n",
    "            torch.sum(z_e_perm ** 2, dim=2, keepdim=True)\n",
    "            - 2 * torch.matmul(z_e_perm, self.codebook.weight.t())\n",
    "            + torch.sum(self.codebook.weight ** 2, dim=1)\n",
    "        )\n",
    "        encoding_indices = torch.argmin(distances, dim=-1)\n",
    "        z_q = self.codebook(encoding_indices)\n",
    "        z_q = z_q.permute(0, 2, 1)  # B x C x T\n",
    "\n",
    "        commitment_loss = F.mse_loss(z_e.detach(), z_q)\n",
    "        codebook_loss = F.mse_loss(z_e, z_q.detach())\n",
    "        loss = commitment_loss + codebook_loss\n",
    "\n",
    "        x_recon = self.decoder(z_q)\n",
    "        return x_recon, loss\n",
    "\n",
    "# ========== Train ========== #\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VQVAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
    "recon_loss_fn = nn.MSELoss()\n",
    "\n",
    "# ========== Learning Rate Scheduler ========== #\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # Decay the LR every 5 epochs\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for mel, _ in train_loader:\n",
    "        mel = mel.to(device)\n",
    "        mel = mel[:, :, :256]  # Ensure fixed size input\n",
    "        x_recon, vq_loss = model(mel)\n",
    "        \n",
    "        # ðŸ”§ Crop output to match input size\n",
    "        min_len = min(x_recon.shape[2], mel.shape[2])\n",
    "        x_recon = x_recon[:, :, :min_len]\n",
    "        mel = mel[:, :, :min_len]\n",
    "\n",
    "        recon_loss = recon_loss_fn(x_recon, mel)\n",
    "        loss = recon_loss + vq_loss\n",
    "        \n",
    "        # Gradient Clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Step the learning rate scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {total_loss:.4f} | LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "# ========== Save Model ========== #\n",
    "torch.save(model.state_dict(), 'vqvae_model.pth')\n",
    "print(\"Model saved as vqvae_model.pth\")\n",
    "\n",
    "# ========== Generate Sample Reconstructions ========== #\n",
    "model.eval()\n",
    "os.makedirs(\"samples\", exist_ok=True)\n",
    "with torch.no_grad():\n",
    "    for i in range(3):  # Save 3 examples\n",
    "        mel, waveform = dataset[i]\n",
    "        mel = mel.unsqueeze(0).to(device)\n",
    "        mel = mel[:, :, :256]\n",
    "        recon_mel, _ = model(mel)\n",
    "        recon_mel = recon_mel.cpu().squeeze(0)\n",
    "\n",
    "        inverse_mel = torchaudio.transforms.InverseMelScale(n_stft=513, n_mels=80, sample_rate=16000)(recon_mel)\n",
    "        griffin_lim = torchaudio.transforms.GriffinLim(n_fft=1024, hop_length=256)\n",
    "        waveform_recon = griffin_lim(inverse_mel)\n",
    "\n",
    "        sf.write(f\"samples/original_{i}.wav\", waveform[:waveform_recon.shape[-1]].numpy(), 16000)\n",
    "        sf.write(f\"samples/reconstructed_{i}.wav\", waveform_recon.numpy(), 16000)\n",
    "        print(f\"Saved: samples/original_{i}.wav and samples/reconstructed_{i}.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32b670d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total audio files found: 3613\n",
      "Epoch 1/50 | Train Loss: 225.0724 | Val Loss: 20.2659 | LR: 0.000200\n",
      "Epoch 2/50 | Train Loss: 176.2235 | Val Loss: 18.7323 | LR: 0.000200\n",
      "Epoch 3/50 | Train Loss: 164.5777 | Val Loss: 17.6183 | LR: 0.000200\n",
      "Epoch 4/50 | Train Loss: 155.6207 | Val Loss: 16.8075 | LR: 0.000200\n",
      "Epoch 5/50 | Train Loss: 148.9674 | Val Loss: 16.1901 | LR: 0.000100\n",
      "Epoch 6/50 | Train Loss: 144.2535 | Val Loss: 15.8508 | LR: 0.000100\n",
      "Epoch 7/50 | Train Loss: 142.2986 | Val Loss: 15.6460 | LR: 0.000100\n",
      "Epoch 8/50 | Train Loss: 140.3610 | Val Loss: 15.4878 | LR: 0.000100\n",
      "Epoch 9/50 | Train Loss: 138.4984 | Val Loss: 15.2205 | LR: 0.000100\n",
      "Epoch 10/50 | Train Loss: 136.6675 | Val Loss: 15.0556 | LR: 0.000050\n",
      "Epoch 11/50 | Train Loss: 135.5017 | Val Loss: 14.9884 | LR: 0.000050\n",
      "Epoch 12/50 | Train Loss: 134.9000 | Val Loss: 14.8529 | LR: 0.000050\n",
      "Epoch 13/50 | Train Loss: 134.0537 | Val Loss: 14.7826 | LR: 0.000050\n",
      "Epoch 14/50 | Train Loss: 132.9637 | Val Loss: 14.7070 | LR: 0.000050\n",
      "Epoch 15/50 | Train Loss: 132.3918 | Val Loss: 14.6854 | LR: 0.000025\n",
      "Epoch 16/50 | Train Loss: 131.9961 | Val Loss: 14.5767 | LR: 0.000025\n",
      "Epoch 17/50 | Train Loss: 131.5760 | Val Loss: 14.5415 | LR: 0.000025\n",
      "Epoch 18/50 | Train Loss: 131.3516 | Val Loss: 14.5026 | LR: 0.000025\n",
      "Epoch 19/50 | Train Loss: 130.9279 | Val Loss: 14.4871 | LR: 0.000025\n",
      "Epoch 20/50 | Train Loss: 130.5869 | Val Loss: 14.4112 | LR: 0.000013\n",
      "Epoch 21/50 | Train Loss: 130.3972 | Val Loss: 14.4200 | LR: 0.000013\n",
      "Epoch 22/50 | Train Loss: 130.1119 | Val Loss: 14.4101 | LR: 0.000013\n",
      "Epoch 23/50 | Train Loss: 130.1828 | Val Loss: 14.3813 | LR: 0.000013\n",
      "Epoch 24/50 | Train Loss: 129.6876 | Val Loss: 14.3727 | LR: 0.000013\n",
      "Epoch 25/50 | Train Loss: 129.6958 | Val Loss: 14.3601 | LR: 0.000006\n",
      "Epoch 26/50 | Train Loss: 129.4962 | Val Loss: 14.3489 | LR: 0.000006\n",
      "Epoch 27/50 | Train Loss: 129.5656 | Val Loss: 14.3339 | LR: 0.000006\n",
      "Epoch 28/50 | Train Loss: 129.3701 | Val Loss: 14.3262 | LR: 0.000006\n",
      "Epoch 29/50 | Train Loss: 129.2630 | Val Loss: 14.3437 | LR: 0.000006\n",
      "Epoch 30/50 | Train Loss: 129.1711 | Val Loss: 14.3200 | LR: 0.000003\n",
      "Epoch 31/50 | Train Loss: 128.9650 | Val Loss: 14.3211 | LR: 0.000003\n",
      "Epoch 32/50 | Train Loss: 129.0016 | Val Loss: 14.2888 | LR: 0.000003\n",
      "Epoch 33/50 | Train Loss: 128.9740 | Val Loss: 14.2686 | LR: 0.000003\n",
      "Epoch 34/50 | Train Loss: 129.1372 | Val Loss: 14.2863 | LR: 0.000003\n",
      "Epoch 35/50 | Train Loss: 128.9929 | Val Loss: 14.3165 | LR: 0.000002\n",
      "Epoch 36/50 | Train Loss: 128.9927 | Val Loss: 14.2880 | LR: 0.000002\n",
      "Epoch 37/50 | Train Loss: 129.1175 | Val Loss: 14.2924 | LR: 0.000002\n",
      "Epoch 38/50 | Train Loss: 128.9266 | Val Loss: 14.2644 | LR: 0.000002\n",
      "Epoch 39/50 | Train Loss: 128.9027 | Val Loss: 14.2925 | LR: 0.000002\n",
      "Epoch 40/50 | Train Loss: 128.9546 | Val Loss: 14.2500 | LR: 0.000001\n",
      "Epoch 41/50 | Train Loss: 128.8295 | Val Loss: 14.2846 | LR: 0.000001\n",
      "Epoch 42/50 | Train Loss: 128.9993 | Val Loss: 14.2660 | LR: 0.000001\n",
      "Epoch 43/50 | Train Loss: 128.7852 | Val Loss: 14.2978 | LR: 0.000001\n",
      "Epoch 44/50 | Train Loss: 128.9955 | Val Loss: 14.2964 | LR: 0.000001\n",
      "Epoch 45/50 | Train Loss: 128.9041 | Val Loss: 14.2706 | LR: 0.000000\n",
      "Epoch 46/50 | Train Loss: 128.8685 | Val Loss: 14.2780 | LR: 0.000000\n",
      "Epoch 47/50 | Train Loss: 128.8338 | Val Loss: 14.2973 | LR: 0.000000\n",
      "Epoch 48/50 | Train Loss: 128.8650 | Val Loss: 14.2632 | LR: 0.000000\n",
      "Epoch 49/50 | Train Loss: 128.7157 | Val Loss: 14.2758 | LR: 0.000000\n",
      "Epoch 50/50 | Train Loss: 128.6313 | Val Loss: 14.2559 | LR: 0.000000\n",
      "Model saved as vqvae_model.pth\n",
      "Saved: samples/original_0.wav and samples/reconstructed_0.wav\n",
      "Saved: samples/original_1.wav and samples/reconstructed_1.wav\n",
      "Saved: samples/original_2.wav and samples/reconstructed_2.wav\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import soundfile as sf\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "\n",
    "# ========== Dataset ========== #\n",
    "class VocalSetDataset(Dataset):\n",
    "    def __init__(self, file_list):\n",
    "        self.files = file_list\n",
    "        self.mel_transform = nn.Sequential(\n",
    "            MelSpectrogram(sample_rate=16000, n_fft=1024, hop_length=256, n_mels=80),\n",
    "            AmplitudeToDB()\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        waveform, sr = torchaudio.load(self.files[idx])\n",
    "        if sr != 16000:\n",
    "            waveform = torchaudio.functional.resample(waveform, sr, 16000)\n",
    "        mel = self.mel_transform(waveform)\n",
    "        mel = mel.mean(dim=0, keepdim=True)  # Convert to mono\n",
    "        mel = (mel - mel.mean()) / (mel.std() + 1e-6)  # Normalize\n",
    "        return mel.squeeze(0), waveform.squeeze(0)\n",
    "\n",
    "# ========== Load Dataset ========== #\n",
    "root_dir = 'VocalSet_processed'\n",
    "all_files = [\n",
    "    os.path.join(root, f)\n",
    "    for root, _, files in os.walk(root_dir)\n",
    "    for f in files if f.endswith('.wav')\n",
    "]\n",
    "\n",
    "print(\"Total audio files found:\", len(all_files))\n",
    "if len(all_files) == 0:\n",
    "    raise RuntimeError(\"No .wav files found. Please check the path and file extensions.\")\n",
    "\n",
    "dataset = VocalSetDataset(all_files)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=16, shuffle=False)\n",
    "\n",
    "# ========== VQ-VAE Model ========== #\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, num_embeddings=512, embedding_dim=64):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(80, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, embedding_dim, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(embedding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.codebook = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(embedding_dim, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(128, 80, kernel_size=4, stride=2, padding=1)\n",
    "        )\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_e = self.encoder(x)  # B x C x T\n",
    "        z_e_perm = z_e.permute(0, 2, 1)  # B x T x C\n",
    "        distances = (\n",
    "            torch.sum(z_e_perm ** 2, dim=2, keepdim=True)\n",
    "            - 2 * torch.matmul(z_e_perm, self.codebook.weight.t())\n",
    "            + torch.sum(self.codebook.weight ** 2, dim=1)\n",
    "        )\n",
    "        encoding_indices = torch.argmin(distances, dim=-1)  # B x T\n",
    "        z_q = self.codebook(encoding_indices)  # B x T x C\n",
    "        z_q = z_q.permute(0, 2, 1)  # B x C x T\n",
    "\n",
    "        commitment_loss = F.mse_loss(z_e.detach(), z_q)\n",
    "        codebook_loss = F.mse_loss(z_e, z_q.detach())\n",
    "\n",
    "        x_recon = self.decoder(z_q)\n",
    "        return x_recon, codebook_loss, commitment_loss\n",
    "\n",
    "# ========== Training Setup ========== #\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VQVAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
    "recon_loss_fn = nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "num_epochs = 50\n",
    "beta = 0.25  # commitment weight\n",
    "\n",
    "# ========== Training Loop ========== #\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for mel, _ in train_loader:\n",
    "        mel = mel.to(device)\n",
    "        mel = mel[:, :, :256]  # Optional: Ensure fixed input size\n",
    "        x_recon, codebook_loss, commitment_loss = model(mel)\n",
    "\n",
    "        # Ensure same length between prediction and ground truth\n",
    "        min_len = min(x_recon.size(2), mel.size(2))\n",
    "        x_recon = x_recon[:, :, :min_len]\n",
    "        mel = mel[:, :, :min_len]\n",
    "\n",
    "        loss_recon = recon_loss_fn(x_recon, mel)\n",
    "        loss = loss_recon + codebook_loss + beta * commitment_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # === Validation ===\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for mel, _ in val_loader:\n",
    "            mel = mel.to(device)\n",
    "            mel = mel[:, :, :256]\n",
    "            x_recon, codebook_loss, commitment_loss = model(mel)\n",
    "\n",
    "            # Match dimensions\n",
    "            min_len = min(x_recon.size(2), mel.size(2))\n",
    "            x_recon = x_recon[:, :, :min_len]\n",
    "            mel = mel[:, :, :min_len]\n",
    "\n",
    "            loss_recon = recon_loss_fn(x_recon, mel)\n",
    "            loss = loss_recon + codebook_loss + beta * commitment_loss\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {total_train_loss:.4f} | Val Loss: {total_val_loss:.4f} | LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "# ========== Save Model ========== #\n",
    "torch.save(model.state_dict(), 'vqvae_model.pth')\n",
    "print(\"Model saved as vqvae_model.pth\")\n",
    "\n",
    "# ========== Generate Sample Reconstructions ========== #\n",
    "model.eval()\n",
    "os.makedirs(\"samples\", exist_ok=True)\n",
    "with torch.no_grad():\n",
    "    for i in range(3):  # Save 3 examples\n",
    "        mel, waveform = dataset[i]\n",
    "        mel = mel.unsqueeze(0).to(device)\n",
    "        mel = mel[:, :, :256]\n",
    "        recon_mel, _, _ = model(mel)\n",
    "        recon_mel = recon_mel.cpu().squeeze(0)\n",
    "\n",
    "        # Convert back to waveform\n",
    "        inverse_mel = torchaudio.transforms.InverseMelScale(n_stft=513, n_mels=80, sample_rate=16000)(recon_mel)\n",
    "        griffin_lim = torchaudio.transforms.GriffinLim(n_fft=1024, hop_length=256)\n",
    "        waveform_recon = griffin_lim(inverse_mel)\n",
    "        waveform_recon = waveform_recon.clamp(-1.0, 1.0)\n",
    "\n",
    "        sf.write(f\"samples/original_{i}.wav\", waveform[:waveform_recon.shape[-1]].numpy(), 16000)\n",
    "        sf.write(f\"samples/reconstructed_{i}.wav\", waveform_recon.numpy(), 16000)\n",
    "        print(f\"Saved: samples/original_{i}.wav and samples/reconstructed_{i}.wav\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62432a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total audio files found: 3613\n",
      "Epoch 1/50 | Train Loss: 248.2712 | Val Loss: 24.4945 | LR: 0.000200\n",
      "Epoch 2/50 | Train Loss: 214.2066 | Val Loss: 22.9923 | LR: 0.000200\n",
      "Epoch 3/50 | Train Loss: 200.6677 | Val Loss: 21.6811 | LR: 0.000200\n",
      "Epoch 4/50 | Train Loss: 190.4460 | Val Loss: 20.6584 | LR: 0.000200\n",
      "Epoch 5/50 | Train Loss: 182.6101 | Val Loss: 19.8794 | LR: 0.000100\n",
      "Epoch 6/50 | Train Loss: 177.9725 | Val Loss: 19.6814 | LR: 0.000100\n",
      "Epoch 7/50 | Train Loss: 175.2751 | Val Loss: 19.3912 | LR: 0.000100\n",
      "Epoch 8/50 | Train Loss: 173.0194 | Val Loss: 19.1428 | LR: 0.000100\n",
      "Epoch 9/50 | Train Loss: 171.6965 | Val Loss: 18.9958 | LR: 0.000100\n",
      "Epoch 10/50 | Train Loss: 170.2377 | Val Loss: 18.8369 | LR: 0.000050\n",
      "Epoch 11/50 | Train Loss: 168.5857 | Val Loss: 18.8689 | LR: 0.000050\n",
      "Epoch 12/50 | Train Loss: 167.9193 | Val Loss: 18.7181 | LR: 0.000050\n",
      "Epoch 13/50 | Train Loss: 167.9771 | Val Loss: 18.6892 | LR: 0.000050\n",
      "Epoch 14/50 | Train Loss: 167.4866 | Val Loss: 18.6538 | LR: 0.000050\n",
      "Epoch 15/50 | Train Loss: 167.9022 | Val Loss: 18.8058 | LR: 0.000025\n",
      "Epoch 16/50 | Train Loss: 168.4326 | Val Loss: 18.7608 | LR: 0.000025\n",
      "Epoch 17/50 | Train Loss: 169.0261 | Val Loss: 18.7604 | LR: 0.000025\n",
      "Epoch 18/50 | Train Loss: 168.5411 | Val Loss: 18.8238 | LR: 0.000025\n",
      "Epoch 19/50 | Train Loss: 168.1427 | Val Loss: 18.8006 | LR: 0.000025\n",
      "Epoch 20/50 | Train Loss: 167.5941 | Val Loss: 18.6453 | LR: 0.000013\n",
      "Epoch 21/50 | Train Loss: 167.4902 | Val Loss: 18.5717 | LR: 0.000013\n",
      "Epoch 22/50 | Train Loss: 167.3142 | Val Loss: 18.6107 | LR: 0.000013\n",
      "Epoch 23/50 | Train Loss: 167.0052 | Val Loss: 18.5616 | LR: 0.000013\n",
      "Epoch 24/50 | Train Loss: 166.9181 | Val Loss: 18.5820 | LR: 0.000013\n",
      "Epoch 25/50 | Train Loss: 166.6369 | Val Loss: 18.5611 | LR: 0.000006\n",
      "Epoch 26/50 | Train Loss: 166.6201 | Val Loss: 18.5391 | LR: 0.000006\n",
      "Epoch 27/50 | Train Loss: 166.2444 | Val Loss: 18.5103 | LR: 0.000006\n",
      "Epoch 28/50 | Train Loss: 166.1885 | Val Loss: 18.5757 | LR: 0.000006\n",
      "Epoch 29/50 | Train Loss: 166.1484 | Val Loss: 18.5371 | LR: 0.000006\n",
      "Epoch 30/50 | Train Loss: 166.0669 | Val Loss: 18.4810 | LR: 0.000003\n",
      "Epoch 31/50 | Train Loss: 165.9651 | Val Loss: 18.4209 | LR: 0.000003\n",
      "Epoch 32/50 | Train Loss: 166.0110 | Val Loss: 18.5051 | LR: 0.000003\n",
      "Epoch 33/50 | Train Loss: 165.9065 | Val Loss: 18.4393 | LR: 0.000003\n",
      "Epoch 34/50 | Train Loss: 165.7568 | Val Loss: 18.5532 | LR: 0.000003\n",
      "Epoch 35/50 | Train Loss: 166.0400 | Val Loss: 18.4329 | LR: 0.000002\n",
      "Epoch 36/50 | Train Loss: 165.6200 | Val Loss: 18.4974 | LR: 0.000002\n",
      "Epoch 37/50 | Train Loss: 165.6470 | Val Loss: 18.4606 | LR: 0.000002\n",
      "Epoch 38/50 | Train Loss: 165.8692 | Val Loss: 18.4642 | LR: 0.000002\n",
      "Epoch 39/50 | Train Loss: 165.5949 | Val Loss: 18.4390 | LR: 0.000002\n",
      "Epoch 40/50 | Train Loss: 165.8109 | Val Loss: 18.4669 | LR: 0.000001\n",
      "Epoch 41/50 | Train Loss: 165.5837 | Val Loss: 18.4384 | LR: 0.000001\n",
      "Epoch 42/50 | Train Loss: 165.6831 | Val Loss: 18.5235 | LR: 0.000001\n",
      "Epoch 43/50 | Train Loss: 165.5642 | Val Loss: 18.5220 | LR: 0.000001\n",
      "Epoch 44/50 | Train Loss: 165.3417 | Val Loss: 18.4539 | LR: 0.000001\n",
      "Epoch 45/50 | Train Loss: 165.5712 | Val Loss: 18.4271 | LR: 0.000000\n",
      "Epoch 46/50 | Train Loss: 165.6175 | Val Loss: 18.4458 | LR: 0.000000\n",
      "Epoch 47/50 | Train Loss: 165.5818 | Val Loss: 18.4257 | LR: 0.000000\n",
      "Epoch 48/50 | Train Loss: 165.6034 | Val Loss: 18.5448 | LR: 0.000000\n",
      "Epoch 49/50 | Train Loss: 165.2903 | Val Loss: 18.3958 | LR: 0.000000\n",
      "Epoch 50/50 | Train Loss: 165.5683 | Val Loss: 18.3723 | LR: 0.000000\n",
      "Model saved as vqvae_model.pth\n",
      "Saved: samples/original_0.wav and samples/reconstructed_0.wav\n",
      "Saved: samples/original_1.wav and samples/reconstructed_1.wav\n",
      "Saved: samples/original_2.wav and samples/reconstructed_2.wav\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import soundfile as sf\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB, Resample\n",
    "\n",
    "# ========== Dataset ========== #\n",
    "class VocalSetDataset(Dataset):\n",
    "    def __init__(self, file_list):\n",
    "        self.files = file_list\n",
    "        self.mel_transform = nn.Sequential(\n",
    "            MelSpectrogram(sample_rate=16000, n_fft=1024, hop_length=256, n_mels=80),\n",
    "            AmplitudeToDB()\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        waveform, sr = torchaudio.load(self.files[idx])\n",
    "        if sr != 16000:\n",
    "            resample = Resample(orig_freq=sr, new_freq=16000)\n",
    "            waveform = resample(waveform)\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)  # Ensure mono\n",
    "        mel = self.mel_transform(waveform)\n",
    "        mel = (mel - mel.mean()) / (mel.std() + 1e-6)  # Normalize\n",
    "        return mel.squeeze(0), waveform.squeeze(0)\n",
    "\n",
    "# ========== Load Dataset ========== #\n",
    "root_dir = 'VocalSet_processed'\n",
    "all_files = [\n",
    "    os.path.join(root, f)\n",
    "    for root, _, files in os.walk(root_dir)\n",
    "    for f in files if f.endswith('.wav')\n",
    "]\n",
    "\n",
    "print(\"Total audio files found:\", len(all_files))\n",
    "if len(all_files) == 0:\n",
    "    raise RuntimeError(\"No .wav files found. Please check the path and file extensions.\")\n",
    "\n",
    "dataset = VocalSetDataset(all_files)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=16, shuffle=False)\n",
    "\n",
    "# ========== VQ-VAE Model ========== #\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, num_embeddings=512, embedding_dim=64):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(80, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, embedding_dim, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(embedding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.codebook = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(embedding_dim, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(128, 80, kernel_size=4, stride=2, padding=1)\n",
    "        )\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_e = self.encoder(x)  # B x C x T\n",
    "        z_e_perm = z_e.permute(0, 2, 1)  # B x T x C\n",
    "        distances = (\n",
    "            torch.sum(z_e_perm ** 2, dim=2, keepdim=True)\n",
    "            - 2 * torch.matmul(z_e_perm, self.codebook.weight.t())\n",
    "            + torch.sum(self.codebook.weight ** 2, dim=1)\n",
    "        )\n",
    "        encoding_indices = torch.argmin(distances, dim=-1)  # B x T\n",
    "        z_q = self.codebook(encoding_indices)  # B x T x C\n",
    "        z_q = z_q.permute(0, 2, 1)  # B x C x T\n",
    "\n",
    "        commitment_loss = F.mse_loss(z_e.detach(), z_q)\n",
    "        codebook_loss = F.mse_loss(z_e, z_q.detach())\n",
    "\n",
    "        x_recon = self.decoder(z_q)\n",
    "        return x_recon, codebook_loss, commitment_loss\n",
    "\n",
    "# ========== Training Setup ========== #\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VQVAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
    "recon_loss_fn = nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "num_epochs = 50\n",
    "beta = 0.25  # commitment weight\n",
    "\n",
    "# ========== Training Loop ========== #\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for mel, _ in train_loader:\n",
    "        mel = mel.to(device)\n",
    "        mel = mel[:, :, :256]  # Fixed input size\n",
    "        x_recon, codebook_loss, commitment_loss = model(mel)\n",
    "\n",
    "        min_len = min(x_recon.size(2), mel.size(2))\n",
    "        x_recon = x_recon[:, :, :min_len]\n",
    "        mel = mel[:, :, :min_len]\n",
    "\n",
    "        loss_recon = recon_loss_fn(x_recon, mel)\n",
    "        loss = loss_recon + codebook_loss + beta * commitment_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # === Validation ===\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for mel, _ in val_loader:\n",
    "            mel = mel.to(device)\n",
    "            mel = mel[:, :, :256]\n",
    "            x_recon, codebook_loss, commitment_loss = model(mel)\n",
    "\n",
    "            min_len = min(x_recon.size(2), mel.size(2))\n",
    "            x_recon = x_recon[:, :, :min_len]\n",
    "            mel = mel[:, :, :min_len]\n",
    "\n",
    "            loss_recon = recon_loss_fn(x_recon, mel)\n",
    "            loss = loss_recon + codebook_loss + beta * commitment_loss\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {total_train_loss:.4f} | Val Loss: {total_val_loss:.4f} | LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "# ========== Save Model ========== #\n",
    "torch.save(model.state_dict(), 'vqvae_model.pth')\n",
    "print(\"Model saved as vqvae_model.pth\")\n",
    "\n",
    "# ========== Generate Sample Reconstructions ========== #\n",
    "model.eval()\n",
    "os.makedirs(\"samples\", exist_ok=True)\n",
    "with torch.no_grad():\n",
    "    for i in range(3):\n",
    "        mel, waveform = dataset[i]\n",
    "        mel = mel.unsqueeze(0).to(device)\n",
    "        mel = mel[:, :, :256]\n",
    "        recon_mel, _, _ = model(mel)\n",
    "        recon_mel = recon_mel.cpu().squeeze(0)\n",
    "\n",
    "        # Inversion\n",
    "        mel_to_linear = torchaudio.transforms.InverseMelScale(\n",
    "            n_stft=513, n_mels=80, sample_rate=16000\n",
    "        )(recon_mel)\n",
    "        griffin_lim = torchaudio.transforms.GriffinLim(\n",
    "            n_fft=1024, hop_length=256\n",
    "        )\n",
    "        waveform_recon = griffin_lim(mel_to_linear)\n",
    "        waveform_recon = waveform_recon.clamp(-1.0, 1.0)\n",
    "\n",
    "        sf.write(f\"samples/original_{i}.wav\", waveform[:waveform_recon.shape[-1]].numpy(), 16000)\n",
    "        sf.write(f\"samples/reconstructed_{i}.wav\", waveform_recon.numpy(), 16000)\n",
    "        print(f\"Saved: samples/original_{i}.wav and samples/reconstructed_{i}.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dc0370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total audio files found: 3613\n",
      "Epoch 1/50 | Train Loss: 237.6294 | Val Loss: 21.6938 | LR: 0.000200\n",
      "Epoch 2/50 | Train Loss: 186.3624 | Val Loss: 20.2336 | LR: 0.000200\n",
      "Epoch 3/50 | Train Loss: 182.3814 | Val Loss: 21.2779 | LR: 0.000200\n",
      "Epoch 4/50 | Train Loss: 187.8439 | Val Loss: 20.7763 | LR: 0.000200\n",
      "Epoch 5/50 | Train Loss: 182.6219 | Val Loss: 20.0683 | LR: 0.000100\n",
      "Epoch 6/50 | Train Loss: 178.7849 | Val Loss: 19.8346 | LR: 0.000100\n",
      "Epoch 7/50 | Train Loss: 176.2482 | Val Loss: 19.6164 | LR: 0.000100\n",
      "Epoch 8/50 | Train Loss: 174.3202 | Val Loss: 19.3451 | LR: 0.000100\n",
      "Epoch 9/50 | Train Loss: 172.5324 | Val Loss: 19.1432 | LR: 0.000100\n",
      "Epoch 10/50 | Train Loss: 170.4913 | Val Loss: 18.9629 | LR: 0.000050\n",
      "Epoch 11/50 | Train Loss: 168.9120 | Val Loss: 18.8438 | LR: 0.000050\n",
      "Epoch 12/50 | Train Loss: 168.0362 | Val Loss: 18.7549 | LR: 0.000050\n",
      "Epoch 13/50 | Train Loss: 167.4956 | Val Loss: 18.6359 | LR: 0.000050\n",
      "Epoch 14/50 | Train Loss: 166.3033 | Val Loss: 18.5807 | LR: 0.000050\n",
      "Epoch 15/50 | Train Loss: 165.9994 | Val Loss: 18.5006 | LR: 0.000025\n",
      "Epoch 16/50 | Train Loss: 165.1648 | Val Loss: 18.4545 | LR: 0.000025\n",
      "Epoch 17/50 | Train Loss: 164.7469 | Val Loss: 18.4036 | LR: 0.000025\n",
      "Epoch 18/50 | Train Loss: 164.3590 | Val Loss: 18.3487 | LR: 0.000025\n",
      "Epoch 19/50 | Train Loss: 164.0120 | Val Loss: 18.3496 | LR: 0.000025\n",
      "Epoch 20/50 | Train Loss: 163.5400 | Val Loss: 18.2656 | LR: 0.000013\n",
      "Epoch 21/50 | Train Loss: 163.3468 | Val Loss: 18.2519 | LR: 0.000013\n",
      "Epoch 22/50 | Train Loss: 163.0541 | Val Loss: 18.2164 | LR: 0.000013\n",
      "Epoch 23/50 | Train Loss: 162.8237 | Val Loss: 18.1931 | LR: 0.000013\n",
      "Epoch 24/50 | Train Loss: 162.7696 | Val Loss: 18.1779 | LR: 0.000013\n",
      "Epoch 25/50 | Train Loss: 162.6018 | Val Loss: 18.2200 | LR: 0.000006\n",
      "Epoch 26/50 | Train Loss: 162.4737 | Val Loss: 18.1706 | LR: 0.000006\n",
      "Epoch 27/50 | Train Loss: 162.3231 | Val Loss: 18.1415 | LR: 0.000006\n",
      "Epoch 28/50 | Train Loss: 162.2438 | Val Loss: 18.1543 | LR: 0.000006\n",
      "Epoch 29/50 | Train Loss: 162.2951 | Val Loss: 18.1469 | LR: 0.000006\n",
      "Epoch 30/50 | Train Loss: 162.0084 | Val Loss: 18.2180 | LR: 0.000003\n",
      "Epoch 31/50 | Train Loss: 161.9450 | Val Loss: 18.1344 | LR: 0.000003\n",
      "Epoch 32/50 | Train Loss: 161.8878 | Val Loss: 18.1154 | LR: 0.000003\n",
      "Epoch 33/50 | Train Loss: 162.0754 | Val Loss: 18.1288 | LR: 0.000003\n",
      "Epoch 34/50 | Train Loss: 162.0568 | Val Loss: 18.1347 | LR: 0.000003\n",
      "Epoch 35/50 | Train Loss: 161.9278 | Val Loss: 18.0950 | LR: 0.000002\n",
      "Epoch 36/50 | Train Loss: 161.9587 | Val Loss: 18.0846 | LR: 0.000002\n",
      "Epoch 37/50 | Train Loss: 161.8724 | Val Loss: 18.0920 | LR: 0.000002\n",
      "Epoch 38/50 | Train Loss: 161.6582 | Val Loss: 18.0816 | LR: 0.000002\n",
      "Epoch 39/50 | Train Loss: 161.6752 | Val Loss: 18.0831 | LR: 0.000002\n",
      "Epoch 40/50 | Train Loss: 161.6695 | Val Loss: 18.0871 | LR: 0.000001\n",
      "Epoch 41/50 | Train Loss: 161.6901 | Val Loss: 18.0757 | LR: 0.000001\n",
      "Epoch 42/50 | Train Loss: 161.6932 | Val Loss: 18.0995 | LR: 0.000001\n",
      "Epoch 43/50 | Train Loss: 161.6134 | Val Loss: 18.1506 | LR: 0.000001\n",
      "Epoch 44/50 | Train Loss: 161.5681 | Val Loss: 18.0744 | LR: 0.000001\n",
      "Epoch 45/50 | Train Loss: 161.6970 | Val Loss: 18.1028 | LR: 0.000000\n",
      "Epoch 46/50 | Train Loss: 161.6796 | Val Loss: 18.0778 | LR: 0.000000\n",
      "Epoch 47/50 | Train Loss: 161.6811 | Val Loss: 18.0647 | LR: 0.000000\n",
      "Epoch 48/50 | Train Loss: 161.7786 | Val Loss: 18.0796 | LR: 0.000000\n",
      "Epoch 49/50 | Train Loss: 161.5210 | Val Loss: 18.0763 | LR: 0.000000\n",
      "Epoch 50/50 | Train Loss: 161.7594 | Val Loss: 18.0879 | LR: 0.000000\n",
      "Model saved as vqvae_model.pth\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 172>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    190\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m    191\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(waveform[:waveform_recon\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\u001b[38;5;241m.\u001b[39mnumpy(), label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal\u001b[39m\u001b[38;5;124m\"\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n\u001b[1;32m--> 192\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mwaveform_recon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReconstructed\u001b[39m\u001b[38;5;124m\"\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n\u001b[0;32m    193\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaveform Comparison - Sample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    194\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSamples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import soundfile as sf\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ========== Dataset ========== #\n",
    "class VocalSetDataset(Dataset):\n",
    "    def __init__(self, file_list):\n",
    "        self.files = file_list\n",
    "        self.mel_transform = nn.Sequential(\n",
    "            MelSpectrogram(sample_rate=16000, n_fft=1024, hop_length=256, n_mels=80),\n",
    "            AmplitudeToDB()\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        waveform, sr = torchaudio.load(self.files[idx])\n",
    "        if sr != 16000:\n",
    "            waveform = torchaudio.functional.resample(waveform, sr, 16000)\n",
    "        mel = self.mel_transform(waveform)\n",
    "        mel = mel.mean(dim=0, keepdim=True)  # Convert to mono\n",
    "        mel = (mel - mel.mean()) / (mel.std() + 1e-6)  # Normalize\n",
    "        return mel.squeeze(0), waveform.squeeze(0)\n",
    "\n",
    "# ========== Load Dataset ========== #\n",
    "root_dir = 'VocalSet_processed'\n",
    "all_files = [\n",
    "    os.path.join(root, f)\n",
    "    for root, _, files in os.walk(root_dir)\n",
    "    for f in files if f.endswith('.wav')\n",
    "]\n",
    "\n",
    "print(\"Total audio files found:\", len(all_files))\n",
    "if len(all_files) == 0:\n",
    "    raise RuntimeError(\"No .wav files found. Please check the path and file extensions.\")\n",
    "\n",
    "dataset = VocalSetDataset(all_files)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=16, shuffle=False)\n",
    "\n",
    "# ========== VQ-VAE Model ========== #\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, num_embeddings=512, embedding_dim=64):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(80, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, embedding_dim, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(embedding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.codebook = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(embedding_dim, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(128, 80, kernel_size=4, stride=2, padding=1)\n",
    "        )\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_e = self.encoder(x)  # B x C x T\n",
    "        z_e_perm = z_e.permute(0, 2, 1)  # B x T x C\n",
    "        distances = (\n",
    "            torch.sum(z_e_perm ** 2, dim=2, keepdim=True)\n",
    "            - 2 * torch.matmul(z_e_perm, self.codebook.weight.t())\n",
    "            + torch.sum(self.codebook.weight ** 2, dim=1)\n",
    "        )\n",
    "        encoding_indices = torch.argmin(distances, dim=-1)  # B x T\n",
    "        z_q = self.codebook(encoding_indices)  # B x T x C\n",
    "        z_q = z_q.permute(0, 2, 1)  # B x C x T\n",
    "\n",
    "        commitment_loss = F.mse_loss(z_e.detach(), z_q)\n",
    "        codebook_loss = F.mse_loss(z_e, z_q.detach())\n",
    "\n",
    "        x_recon = self.decoder(z_q)\n",
    "        return x_recon, codebook_loss, commitment_loss\n",
    "\n",
    "# ========== Training Setup ========== #\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VQVAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
    "recon_loss_fn = nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "num_epochs = 50\n",
    "beta = 0.25  # commitment weight\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# ========== Training Loop ========== #\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for mel, _ in train_loader:\n",
    "        mel = mel.to(device)\n",
    "        mel = mel[:, :, :256]\n",
    "        x_recon, codebook_loss, commitment_loss = model(mel)\n",
    "\n",
    "        min_len = min(x_recon.size(2), mel.size(2))\n",
    "        x_recon = x_recon[:, :, :min_len]\n",
    "        mel = mel[:, :, :min_len]\n",
    "\n",
    "        loss_recon = recon_loss_fn(x_recon, mel)\n",
    "        loss = loss_recon + codebook_loss + beta * commitment_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # === Validation ===\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for mel, _ in val_loader:\n",
    "            mel = mel.to(device)\n",
    "            mel = mel[:, :, :256]\n",
    "            x_recon, codebook_loss, commitment_loss = model(mel)\n",
    "\n",
    "            min_len = min(x_recon.size(2), mel.size(2))\n",
    "            x_recon = x_recon[:, :, :min_len]\n",
    "            mel = mel[:, :, :min_len]\n",
    "\n",
    "            loss_recon = recon_loss_fn(x_recon, mel)\n",
    "            loss = loss_recon + codebook_loss + beta * commitment_loss\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    train_losses.append(total_train_loss)\n",
    "    val_losses.append(total_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {total_train_loss:.4f} | Val Loss: {total_val_loss:.4f} | LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "# ========== Save Model ========== #\n",
    "torch.save(model.state_dict(), 'vqvae_model.pth')\n",
    "print(\"Model saved as vqvae_model.pth\")\n",
    "\n",
    "# ========== Plot Loss Curves ========== #\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(\"loss_curve.png\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
